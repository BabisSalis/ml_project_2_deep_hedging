{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/Repos/ml_project_2_deep_hedging/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from src.env import StockTradingEnv\n",
    "from src.agent import DDPG_Hedger\n",
    "from src.network import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02341482376481996"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"model/hypparams.json\", \"r\") as file:\n",
    "    hyp_params = json.load(file)\n",
    "\n",
    "env = StockTradingEnv(reset_path=True)\n",
    "\n",
    "nHidden = hyp_params[\"hidden_dim\"]\n",
    "actor_lr = 10 ** hyp_params[\"actor_lr\"]\n",
    "critic_lr = 10 ** hyp_params[\"critic_lr\"]\n",
    "trg_update = hyp_params[\"polyak_update_freq\"]\n",
    "nState, nAction = env.observation_space.shape[0], env.action_space.shape[0]  # 3, 1\n",
    "\n",
    "actor_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "from src.buffer import ExpReplay\n",
    "from collections import namedtuple\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class DDPG_Hedger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        Actor: nn.Module,\n",
    "        Critic_1: nn.Module,\n",
    "        Critic_2: nn.Module,\n",
    "        actor_lr: float,\n",
    "        critic_lr: float,\n",
    "        disc_rate: float = 1,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "\n",
    "        # params\n",
    "        self.gamma = disc_rate\n",
    "        self.tau = 0.01\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # experience replay related\n",
    "        self.transition = namedtuple(\n",
    "            \"Transition\",\n",
    "            (\"state\", \"action\", \"reward\", \"next_state\", \"done\"),\n",
    "        )\n",
    "        self.buffer = ExpReplay(10000, self.transition)\n",
    "\n",
    "        # define actor and critic ANN.\n",
    "        self.actor = Actor\n",
    "        self.critic_1 = Critic_1  # mean(cost)\n",
    "        self.critic_2 = Critic_2  # std(cost)\n",
    "\n",
    "        # loss function for critic\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "\n",
    "        # define optimizer for Actor and Critic network\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n",
    "\n",
    "        # define target network needed for DDPG optimization\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.critic_1_target = deepcopy(self.critic_1)\n",
    "        self.critic_2_target = deepcopy(self.critic_2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def store(self, *args):\n",
    "        self.buffer.store(*args)\n",
    "\n",
    "    def act(self, state: list, sigma: float = 0.2):\n",
    "        \"\"\"\n",
    "        We use policy function to find the deterministic action instead of distributions\n",
    "        which is parametrized by distribution parameters learned from the policy.\n",
    "\n",
    "        Here, state input prompts policy network to output a single or multiple-dim\n",
    "        actions.\n",
    "        :param state:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = torch.tensor(state).to(torch.float64)\n",
    "        action = self.actor.forward(x)\n",
    "        noise = Normal(torch.tensor([0.0]), torch.tensor([sigma])).sample().item()\n",
    "        return (\n",
    "            torch.clip((action - 0.5) * 2 + noise, -state[0], 1.0 - state[0])\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "    def update(self, output=False):\n",
    "        # calculate return of all times in the episode\n",
    "        if self.buffer.len() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.transition(*zip(*transitions))\n",
    "\n",
    "        # extract variables from sampled batch.\n",
    "        states = torch.tensor(batch.state)\n",
    "        actions = torch.tensor(batch.action)\n",
    "        rewards = torch.tensor(batch.reward)\n",
    "        dones = torch.tensor(batch.done).float()\n",
    "        next_states = torch.tensor(batch.next_state)\n",
    "\n",
    "        # compute Q_1 loss\n",
    "        Q_1 = self.critic_1(torch.hstack([states, actions]))\n",
    "        y_1 = rewards + self.gamma * (1 - dones) * self.critic_1_target(\n",
    "            torch.hstack([next_states, self.actor_target(next_states)]).detach()\n",
    "        )\n",
    "\n",
    "        critic_loss_1 = self.critic_loss(Q_1, y_1)\n",
    "        \n",
    "        # Optimize the critic Q_1\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_loss_1.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "\n",
    "        # compute Q_2 loss\n",
    "        Q_2 = self.critic_2(torch.hstack([states, actions]))\n",
    "        y_2 = (\n",
    "            rewards**2\n",
    "            + (self.gamma**2)\n",
    "            * (1 - dones)\n",
    "            * self.critic_2_target(\n",
    "                torch.hstack([next_states, self.actor_target(next_states)]).detach()\n",
    "            )\n",
    "            + 2\n",
    "            * self.gamma\n",
    "            * rewards\n",
    "            * self.critic_1_target(\n",
    "                torch.hstack([next_states, self.actor_target(next_states)]).detach()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        critic_loss_2 = self.critic_loss(Q_2, y_2)\n",
    "        \n",
    "        # Optimize the critic Q_2\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_loss_2.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "\n",
    "        # Get actor loss\n",
    "        state_action = torch.hstack([states, self.actor(states)])\n",
    "        cost_variance = (\n",
    "            self.critic_2(state_action)\n",
    "            - self.critic_1(state_action) ** 2\n",
    "        )\n",
    "        #print(self.critic_1(state_action)[:3], self.critic_2(state_action)[:3])\n",
    "        actor_loss = (\n",
    "            self.critic_1(state_action)\n",
    "            + 1.5 * torch.sqrt(torch.where(cost_variance < 0, 0, cost_variance))\n",
    "        ).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        #print(critic_loss_1, critic_loss_2, actor_loss)\n",
    "\n",
    "        if output:\n",
    "            return actor_loss.detach().item()\n",
    "\n",
    "    def polyak_update(self):\n",
    "        # Update the frozen target models\n",
    "        for trg_param, src_param in zip(\n",
    "            list(self.critic_1_target.parameters()), list(self.critic_1.parameters())\n",
    "        ):\n",
    "            trg_param = trg_param * (1.0 - self.tau) + src_param * self.tau\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for trg_param, src_param in zip(\n",
    "            list(self.critic_2_target.parameters()), list(self.critic_2.parameters())\n",
    "        ):\n",
    "            trg_param = trg_param * (1.0 - self.tau) + src_param * self.tau\n",
    "\n",
    "\n",
    "        for trg_param, src_param in zip(\n",
    "            list(self.actor_target.parameters()), list(self.actor.parameters())\n",
    "        ):\n",
    "            trg_param = trg_param * (1.0 - self.tau) + src_param * self.tau\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_hyp_params\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_hyp_params\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(\n",
    "            torch.load(filename + \"_critic_hyp_params\")\n",
    "        )\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_hyp_params\"))\n",
    "\n",
    "        # define target network needed for DDPG optimization\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.critic_target = deepcopy(self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_EPISODE = 20\n",
    "\n",
    "actor = MLP(nState, nHidden, nAction, \"Sigmoid\")\n",
    "qnet_1 = MLP(nState + nAction, nHidden, nAction, \"\")\n",
    "qnet_2 = MLP(nState + nAction, nHidden, nAction, \"\")\n",
    "agent = DDPG_Hedger(actor, qnet_1, qnet_2, actor_lr, critic_lr, 1, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: [35.93762975]\n",
      "Episode 1 Reward: [26.79992093]\n",
      "Episode 2 Reward: [26.79949243]\n",
      "Episode 3 Reward: [26.79953819]\n",
      "Episode 4 Reward: [26.79949243]\n",
      "Episode 5 Reward: [26.79949243]\n",
      "Episode 6 Reward: [26.79962754]\n",
      "Episode 7 Reward: [26.79949243]\n",
      "Episode 8 Reward: [26.79965998]\n",
      "Episode 9 Reward: [26.79950071]\n",
      "Episode 10 Reward: [26.79949536]\n",
      "Episode 11 Reward: [26.79949243]\n",
      "Episode 12 Reward: [26.79949243]\n",
      "Episode 13 Reward: [26.79949243]\n",
      "Episode 14 Reward: [26.79949243]\n",
      "Episode 15 Reward: [26.79949243]\n",
      "Episode 16 Reward: [26.79955142]\n",
      "Episode 17 Reward: [26.79949243]\n",
      "Episode 18 Reward: [26.79953787]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m ep_tot_reward \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     25\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m---> 27\u001b[0m agent\u001b[39m.\u001b[39;49mupdate()\n\u001b[1;32m     29\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[7], line 145\u001b[0m, in \u001b[0;36mDDPG_Hedger.update\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m# Optimize the actor\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 145\u001b[0m actor_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    147\u001b[0m \u001b[39m#print(critic_loss_1, critic_loss_2, actor_loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Repos/ml_project_2_deep_hedging/venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Repos/ml_project_2_deep_hedging/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_rewards = []\n",
    "noise_std = 0.5\n",
    "\n",
    "for episode in range(N_EPISODE):\n",
    "    # reset state\n",
    "    state = env.reset()  # s_0\n",
    "    ep_tot_reward = 0\n",
    "\n",
    "    if episode > N_EPISODE - 30:\n",
    "        noise_std = 0.0001\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        # take action given state\n",
    "        action = agent.act(state, noise_std)\n",
    "        #print(f'------- step {i+1} action: {action}')\n",
    "        \n",
    "        # take next step of the environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # record interaction between environment and the agent\n",
    "        agent.store(state, action, reward, next_state, done)\n",
    "\n",
    "        ep_tot_reward -= reward\n",
    "        state = next_state\n",
    "        \n",
    "        agent.update()\n",
    "\n",
    "        i +=1 \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(f\"Episode {episode} Reward: {ep_tot_reward}\")\n",
    "    # store total rewards after some training is done\n",
    "    # we only consider alst 10 total rewards as a quantity to minimize\n",
    "    if episode > N_EPISODE - 30:\n",
    "        target_rewards.append(ep_tot_reward)\n",
    "\n",
    "    if episode % trg_update == 0:  # update target network\n",
    "        agent.polyak_update()\n",
    "\n",
    "#print(np.mean(target_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(138.7283, grad_fn=<MseLossBackward0>) tensor(570.2459, grad_fn=<MseLossBackward0>) tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agent.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the init params of actor and two critics\n",
    "print('Actor')\n",
    "for param in agent.actor.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('Critic_1')\n",
    "for param in agent.critic_1.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('Critic_2')\n",
    "for param in agent.critic_2.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baf7a57c528b90bfd786fcd04486c8c2dfc7aaff4a71b2812269de64341c9123"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
