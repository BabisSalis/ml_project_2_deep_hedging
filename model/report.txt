------------- 1 -------------
params used
- actor rate, critic rates are 10 ** -4
- batch size = 16, episodes 1000.
- epsilon decay = 0.997

this is the very first training done. We did not use hyperparams trained by optuna.
we saw that the rewards dont increase towards zero but diverging.

maybe increase the batch size 32 and use optuna hyperparams.
------------- 2 -------------
params used
- actor rate, critic rates are set from optuna hyper parameters.
- batch size = 32, episodes 1000.
- epsilon decay = 0.997

As we trained its hyperparameters but model did not converge. 
Current agent outputs actions from -1 to 1. We could change the action as number of holdings literally.
Next time, we will run the model by changing the action formula.
------------- 3 -------------
params used
- actor rate, critic rates equal to 10 ** -4
- batch size = 32, episodes 1000.
- epsilon decay = 0.997

As we trained its hyperparameters but model did not converge. 
Current agent outputs actions from -1 to 1. We could change the action as number of holdings literally.
Next time, we will run the model by changing the action formula.

>>> result
we changed the action equal to holding not amount to buy/sell. however reward does not increase as training episode increases. See the action values.
it is mostly zero (no hedging at all). We should either give a lot of weight to transaction cost since the goal of this is to reduce hedging cost as well the agent does actual hedging.

maybe, rewards are too small for actor network to update. since one contract deals 100 stocks, we can increase the size of rewards. we will try this in next run. 
------------- 4 -------------
kept same params as v3.

changed the magnitude of action = action * 100 to account that 1 option contract has 100 stocks.

------------- 5 -------------
same setting as v4.

but we modified the reward function at the end of episode. we included call option payoff at maturity instead of next call price.

------------- 7 -------------
NORMALIZED THE STATES for calculating the states and updating the network.
I (kibeom) encourage reader to run about 5000 episode with appropriate epsilon decay. training over that amount is overkill and
the agent outputs values like 100 or 0.

------------- 8 -------------
same setting as v7 but ran 1000000 episodes to train weights.

------------- 8_new ---------
network: 

changed network layer order.
ReLU -> LayerNorm (X)
ReLU -> LayerNorm (X)


------------- 9 ---------
increased batch size to 1024
main.py saves net params with lowest actor loss

then we use transfer learning, where we load pretrained params first then
optimize